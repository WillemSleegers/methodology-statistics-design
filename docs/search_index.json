[["introduction-and-overview.html", "Rethink Priorities on surveys, experiments, and data analysis; methodology, protocols and templates 1 Introduction and overview 1.1 Rethink Priorities: Our approach to empirical and quantitative work 1.2 The purpose of this resource 1.3 Sections and organization", " Rethink Priorities on surveys, experiments, and data analysis; methodology, protocols and templates Rethink Priorities researchers, especially the ‘Survey team’ (project started by David Reinstein) 2021-11-15 Abstract A set of curated resources tied to use-cases, and a space to organize our discussion 1 Introduction and overview NOTE: this is now publicly hosted but not indexed. Please be careful not to share any confidential data or sensitive information. 1.1 Rethink Priorities: Our approach to empirical and quantitative work 1.2 The purpose of this resource A github-hosted ‘bookdown’ for structured in-depth discussion David Reinstein’s conception: Don’t share any confidential data or sensitive information Don’t feel compelled to flesh out all sections with original content. Don’t add content just because \"it’s in a typical syllabus’. Focus on things that we use, have used, want to use, or have been requested to address. Secondarily, on things relevant to Effective Altruism-related research in general Do curate link, and embed resources from elsewhere Incorporate examples from our work (where these are not sensitive or they where can be made anonymous) Do put in content that is more in-depth and technical, or involving R-code and tools Still, do try to ‘offset’ details (in folding blocks, appendix sections, margin notes), where it would otherwise clutter the book Start with ‘plain language’ explanations of technical content; for ourselves, and potentially to share with partners in future I also hope to use this to develop ‘templates and standard practices’ for our work 1.3 Sections and organization (Proposed/discussedbelow – keep updated with actual structure) DATA, CODE * * Note that the capital letters denote the highest-level sections, but in the markdown these are at the same level as the sections below Coding practice and tools (Languages, clean code, reproducability, etc.) Data practices (Storing, labeling, etc) PRESENTING AND DESCRIBING DATA Methods of ‘describing data’ Formatting: tables, figures, and numerical content** Visualizations: suggested/preferred formats and templates SURVEY DESIGNS AND METHODS How to ask good survey questions Avoiding pitfalls Sampling issues and representativeness Constructing reliable indices and scales Implementation platforms and issues Survey design tools (IT) **EXPERIMENTS AND TRIALS* Experiment and trial design: qualitative issues and guidelines Experiment and trial design: quantitative issues ‘Treatment’ assignment (blocking, randomization, etc) Adaptive, sequential and dynamic designs Planning, diagnosing and adjusting a design Power analyses (and other ‘diagnosands’) BASIC STATISTICS: MODELING, TESTING AND INFERENCE Bayesian, frequentist, and other approaches A ‘statistical model’* * I put in the first discussion of ‘models’ here because, at least in one perspective, all of the below is based on models. Bayesian updating and inference Hypothesis testing Preferred approaches (‘which tests’) etc. MODELING, PREDICTION, INFERENCE, AND MACHINE LEARNING “Multi-variable ‘regression’ models” and specification choices Interpreting model results Predictive modeling and machine learning Practical Bayesian approaches and interpretations Psychometrics, especially factor analysis CAUSAL INFERENCE Basic ideas and frameworks (simple, potential outcomes, DAGs) Pitfalls and mistakes (layman’s terms) The experimental ideal Non-experimental approaches to causal inference Dealing with attrition MONTE-CARLO ‘FERMI ESTIMATION’ APPROACHES; The basic ideas Causal and Guesstimate Code-based tools } "],["data.html", "DATA, CODE, AND PRESENTATION", " DATA, CODE, AND PRESENTATION "],["coding-etc.html", "2 Coding practice and tools", " 2 Coding practice and tools "],["data-practices.html", "3 Data practices", " 3 Data practices #PRESENTING AND DESCRIBING DATA {-#present] "],["methods-of-describing-data.html", "4 Methods of ‘describing data’", " 4 Methods of ‘describing data’ "],["formatting-tables-figures-and-numerical-content.html", "5 Formatting: tables, figures, and numerical content", " 5 Formatting: tables, figures, and numerical content "],["visualizations.html", "6 Visualizations 6.1 Suggested/preferred formats and templates", " 6 Visualizations 6.1 Suggested/preferred formats and templates "],["ea-survey-presentation-methodology.html", "7 EA Survey: Presentation, methodology 7.1 Organisation, collophon, formatting 7.2 Coding and organisational issues 7.3 Content and methods", " 7 EA Survey: Presentation, methodology Note: this has been moved from the ea-data repo, and is being expanded here 7.1 Organisation, collophon, formatting 7.1.1 How this is built, how it works, how it relates to EA Forum posts 7.1.2 Markdown formatting, conversion, etc. We need to convert this Rmd/html ‘bookdown’ output into a format acceptable for the EA Forum (however, we can also separately link and host this bookdown, or parts of it, as we do HERE. Typically, RP has made these posts first in Google Docs for feedback and then there is some sort of sprocedure to get it into EA forum markdown syntax from there. For some previous posts, we pasted from the Rstudio visual mode pastes into Google docs for narrative, and figures/tables could be manually pasted. But we want to avoid having to do that! We hope this can be done through simple script Pandoc plus a simple adjustment script (in Python or Vim etc). Some formatting adjustments will be necessary, however: EA Forum doesn’t have margin notes or folding boxes. Margin notes should be made into footnotes. (Or notes in parentheses if they need to be prominent). Folded boxes should probably be retained in the (linked) hosted bookdown only. NOTE: remaining content moved to methodology repo 7.2 Coding and organisational issues 7.3 Content and methods 7.3.1 Visualisations See also Gdoc: visualisation discussions 7.3.1.1 Ideal coding practice for visualisations Use ggplot always unless you’ve a strong reason to do otherwise. Always save the plot as an object in the environment as well as printing it where it belongs. Ideally, we will come up with a set of functions and default options for the key sets of plots we use and like. Define and reuse lists of labels (or use inherent labeling options where these exist, to exploit existing variable and value labels.) Otherwise define sets of common options for plots in one place, and repeated ggplot elements together as a list: to avoid repetition and clutter to allow easy ‘global’ adjustment 7.3.1.2 Labeling Use readable but concise labels. “Amount donated” is OK, “GWWC” is OK because we assume people know what GWWC is from context. amt_don is not OK. 7.3.1.3 Relative shares of a total; e.g. ‘shares of EA population by referrer,’ or ‘donation totals by country’ Treemaps NOTE: The below will not run without data, but we cannot make data ‘public’ here … we need to make some choices … or maybe use simulated data in this discussion geom_treemap_opts &lt;- list(geom_treemap(alpha = 0.7), geom_treemap_text(fontface = &quot;italic&quot;, colour = &quot;white&quot;, place = &quot;centre&quot;, grow = TRUE, min.size = 1 ), theme(legend.position = &quot;none&quot;, plot.title = element_text(hjust = 0.5)) ) ## Error in geom_treemap(alpha = 0.7): could not find function &quot;geom_treemap&quot; ( don_share_by_size &lt;- eas_20 %&gt;% select(donation_2019_c, donation_2019_c_split) %&gt;% group_by(donation_2019_c_split) %&gt;% summarise(total_don = sum(donation_2019_c, na.rm=TRUE)) %&gt;% mutate(don_share = round(total_don/sum(total_don)*100)) %&gt;% filter(!is.na(donation_2019_c_split)) %&gt;% ggplot(aes(area = total_don, fill= donation_2019_c_split, # Include percentage of total donation label = paste(donation_2019_c_split, paste0(don_share, &quot;%&quot;), sep = &quot;\\n&quot;))) + geom_treemap_opts + ggtitle(&quot;Share of total 2019 reported donation amount, by donation size&quot;) ) ## Error in select(., donation_2019_c, donation_2019_c_split): object &#39;eas_20&#39; not found A good alternative to the treemap … conveying both absolute amounts and rates in the same graph? 7.3.1.4 Likert scale (individual items) and relative responses to these 7.3.1.5 Dealing with scales and large differences in magnitudes If axes are cut and don’t go to zero, mention this prominently in the plot. But what if we have a few values that are way above others, and ‘mess up the scale?’ Sometimes a logarithmic scale is helpful, but that can also lead to confusion. Otherwise… “any CI’s or other things that go outside of the limits should just be extended to the edge and not dropped)” doable with scale_y_continuous( oob = scales::squish) + scale_x_continuous( oob = scales::squish)or `coord_cartesian` or otherwise allow a clear ‘break’ in the axis? Not possible to break the axis ggplot https://stackoverflow.com/questions/7194688/using-ggplot2-can-i-insert-a-break-in-the-axis (Oska?) – what about ‘extending to the edge?’ Remove outliers as a last resort; prominently mention this where doing so. 7.3.1.6 Means of continuous variables: differences across groups 7.3.1.7 Visualizing (regression) model coefficients and confidence intervals Testing a reference Wickham (2010) 7.3.2 Statistical tests (make a new section)s 7.3.3 ‘Models’ Contrast: Descriptive (and perhaps causally suggestive), aka ‘dimension reduction’ Predictive Causal See discussion in Engagement section here 7.3.4 Model coding and implementation practice Define lists of variables (features) up front (see, e.g., in engagement post ‘Choosing features…’) outcomes, ‘predictors’ or associated variables of interest, (define broad and narrow sets, consider endogeneity of various forms, e.g., ‘colliders,’ consider ‘clean model without leacks’ for specific prediction exercises) ‘control variables’ (not of direct interest but included because they are believed to improve interpretation of variables of interest; note this can be problematic), ‘robust controls’ for more flexible control E.g., cat_out &lt;- &quot;engagement&quot; #categorical outcome key_demog &lt;- c(&quot;age_approx_d2sd&quot;, &quot;gender_cat&quot;, &quot;student_cat&quot;, &quot;race_cat&quot;, geog) controls &lt;- c(&quot;years_involved_d2sd&quot;) Missing values: Impute or code as separate ‘NA’ category Normalize continuous variables (de-mean, divide continuous variables by 2sd if comparing coefficients to binary variables) Categorical/discrete variables: References "],["survey-designs-and-methods.html", "SURVEY DESIGNS AND METHODS", " SURVEY DESIGNS AND METHODS Schwarcz et al. (2007) References "],["surveys.html", "SURVEY DESIGNS AND METHODS 7.4 How to ask good survey questions 7.5 Avoiding pitfalls 7.6 Constructing reliable indices and scales 7.7 Sampling issues and representativeness 7.8 Implementation platforms and issues 7.9 Survey design tools (IT)", " SURVEY DESIGNS AND METHODS This section will cover specific ‘survey’ related content, as well as some content that overlaps with experiments and trials. More technical and involved discussion of overlapping content will often be deferred to the experiments and trials section, with placeholders and links. Schwarcz et al. (2007) 7.4 How to ask good survey questions 7.5 Avoiding pitfalls 7.6 Constructing reliable indices and scales 7.7 Sampling issues and representativeness 7.8 Implementation platforms and issues 7.9 Survey design tools (IT) References "],["expt_trial.html", "EXPERIMENTS AND TRIAL DESIGN", " EXPERIMENTS AND TRIAL DESIGN Note that much ‘experiment and trial’ relevant content is covered in the surveys section. We will put in placeholders and cross-link. "],["expt-qual-imp.html", "8 Experiment and trial design: qualitative issues and guidelines, implementation issues 8.1 Basic design choices and terminology 8.2 Hypothesis testing versus ‘reinforcement learning’ goals 8.3 Design concerns and pitfalls 8.4 Practical and implementation issues", " 8 Experiment and trial design: qualitative issues and guidelines, implementation issues 8.1 Basic design choices and terminology 8.1.1 Types of experiments: ‘lab and field’ ## Formulating hypotheses and ‘learning and adaptation goals’ 8.2 Hypothesis testing versus ‘reinforcement learning’ goals 8.3 Design concerns and pitfalls 8.3.1 Confounded designs 8.3.2 Attrition and failed randomization 8.3.3 ‘Demand effects’ 8.3.4 Naturalness versus cleanliness 8.4 Practical and implementation issues Survey and experiment platforms (see ‘surveys’) chapter 8.4.1 Field experiments, A/B and lift tests, and marketing trials 8.4.2 Designing, coding, and implementing experiments: IT issues 8.4.3 Failure and success modes 8.4.4 Capturing outcome data 8.4.5 Pre-registration and pre-analysis plans "],["expt-quant.html", "9 Experiment and trial design: quantitative issues 9.1 ‘Treatment’ assignment (blocking, randomization, etc) 9.2 Planning, diagnosing and adjusting a design 9.3 Power analyses (and other ‘diagnosands’)", " 9 Experiment and trial design: quantitative issues 9.1 ‘Treatment’ assignment (blocking, randomization, etc) 9.1.1 Adaptive, sequential and dynamic designs 9.2 Planning, diagnosing and adjusting a design 9.2.1 Specifying models, hypotheses, and statistical testing and inference approaches (Just a brief here, this will be covered in more detail in the sections on statistical inference, testing, and analysis 9.3 Power analyses (and other ‘diagnosands’) Note: this will very much overlap the discussion of power analysis for surveys. Perhaps we put the simpler introduction there, and the more involved details here 9.3.1 Key resources and explainers (‘Curate link’ tools we found useful) 9.3.2 See ‘Rethink_Priorities_Power_analysis_framework_2’ in appendix This is a proposal and tools for a ‘path to do’ power testing (by Jamie Elsey; David Reinstein will weigh in too). I think this first focuses on a frequentist approach, but it’s likely to also introduce a Bayesian approach. DR: I’m putting it into an appendix to the bookdown for now. We may incorporate much of that into the present section. An alternative to an ‘appendix’ is a standalone page which we can link here. This can sometimes be helpful; it allows a different format, may save time on knitting, may allow that other page to be better integrated elsewhere. But for now, I’ll make it a bookdown appendix, to help get us all used to the current format and avoid disagregation. "],["basic-statistics-modeling-testing-and-inference.html", "BASIC STATISTICS: MODELING, TESTING AND INFERENCE", " BASIC STATISTICS: MODELING, TESTING AND INFERENCE "],["modeling.html", "MODELING, PREDICTION, INFERENCE, AND MACHINE LEARNING", " MODELING, PREDICTION, INFERENCE, AND MACHINE LEARNING "],["causal.html", "CAUSAL INFERENCE", " CAUSAL INFERENCE "],["fermi.html", "MONTE-CARLO ‘FERMI ESTIMATION’ APPROACHES*", " MONTE-CARLO ‘FERMI ESTIMATION’ APPROACHES* "],["appendixsupplements.html", "10 APPENDIX/SUPPLEMENTS", " 10 APPENDIX/SUPPLEMENTS "],["power-workflow.html", "11 Power analysis workflow (Jamie Elsey) 11.1 Discussion and framework 11.2 Concrete implementation of framework", " 11 Power analysis workflow (Jamie Elsey) 11.1 Discussion and framework 11.1.1 Introduction and goals The purpose of this document is to propose the fundamentals for a workflow for power analyses. Although this started from a Bayesian paradigm, the overarching framework is applicable to any analytic approach, and I will begin with a standard frequentist analysis.* * This will make the code easier to run as well. Note that as Bayesian power analyses can be very time consuming (at the moment/with our current setup), in some cases even if you ultimately might perform a Bayesian analysis, it may make some sense to run frequentist approaches first. In many cases the estimates from Bayesian and frequentist approaches will tend to converge. I (Jamie) suspect general estimates from frequentist approaches would be quite similar to those of Bayesian approaches, if the goal of the analysis (what you want to make an inference about) is the same. I will make an accompanying document with an example of a Bayesian power analysis that should be computationally feasible to accompany this. The general approach laid out here can be the basis for developing and build a library of common analyses and inference goals. (DR: we might want to incorporate declaredesign for this.) Helpful future additions to the toolkit would include clear and flexible ways to generate hypothetical data sets, and adding further analytic designs. This is not intended as an exhaustive introduction to the fundamentals of statistical power - I assume you are reasonably well-versed in some general principles of statistical inference and hypothesis testing, as well as in the basic idea of what power analysis is. (DR: We can cover this in the earlier section, or link relevant expanations). We will focus solely on ‘simulation-based’ power analysis, and not on ways of mathematically (analytically) deriving power. 11.1.2 Definition of power In frequentist null hypothesis significance testing (NHST), power is typically defined as the probability that we can reject the null hypothesis, if the alternative hypothesis is indeed true (i.e., power is the ‘true positive rate’).* * More precisely, we may express this as ‘power against a particular alternative hypothesis.’ (DR: may be worth putting maths here at some point, and also give a canonical reference.) For Bayesian analyses, there is some esoteric discussion about ‘whether power is even a thing we should be discussing.’ There is uncertainty about the data-generating process, but there is only one data set.* *Maybe add ‘and there is no standard ’null hypothesis testing procedure.’ Actually, maybe this point needs a bit more fleshing out? What is the implication of ‘there is only one data set?’ Before we conduct an experiment (or run a survey, or collect data for an analysis), there are many hypothetical future data sets we might observe. Whether we favor a frequentist or Bayesian approach, it seems reasonable to ask: ‘Given the range of data that I might observe, how likely is it that I can make certain conclusions?’. The conclusion or inference one wishes to make from a data set can go far beyond simply saying something like “there is a non-zero difference between groups”** (although this is probably the most common inference people first consider). ** Or a comparable statement posed in probablistic terms… Frequentist: ‘if there had been a zero difference between these groups (H0), this data is unlikely to have been generated.’ Bayesian: ‘the posterior distribution puts most of the probability mass on there being a difference between groups greater than (some moderate amount).’ We can broaden the idea of power to indicate the probability that our proposed sample yields information that allows us to make some specific kind of inference about the data-generating process. These inferences can take many forms, such as:* DR: Which of these do you consider frequentist and which are Bayesian, and which can be either? the probability of determining that there is a non-zero difference between two conditions, the probability of detecting an effect in a regression model of a specific magnitude (or greater), the probability of detecting some ‘smallest effect size of interest,’ the probability of a ‘false positive,’ i.e., the probability of concluding that there is a difference between groups when in fact there is no difference, (DR: I didn’t think this would ever be called ‘power’ .. I thought this would be called the ‘size’ of the test), or the probability of achieving a desired level of precision around a particular parameter estimate. Hence, a power analysis helps frame our prospective research project in relation to the goals we would like to achieve (DR: cf the ‘diagnosands’ of the declaredesign framework). It provides us us with an estimate (and it is indeed an estimate, not a certainty!) of the probability that we will be able to make the inference we wish to make, given various factors both inside and outside of our control. (See discussion in fold.) What is ‘in our control?’ We might think of sample size as in our control, which it generally is, but this is typically limited by practical considerations outside of our control (such as the availability of a sample or financial constraints). Conversely, effect size is often considered out of our contro;; however we can sometimes increase the ‘dosage’ we give, or try to select a particular group of participants who might be particularly susceptible to our effect of interest. 11.1.3 General workflow for simulation-based power analysis Simulation-based power analysis proceeds in four primary steps: Generate a large number of ‘simulated data sets’ for the analysis.* * These may be generated by draws based on a canonical distribution, like the Gaussian (‘normal’) with particular parameters. Alternately, if prior ‘similar’ data is available (e.g., from outcomes in the year prior to a field experiment), we may prefer to generate it by resampling from this. The data sets generated should be specific to the goal of the study. E.g., suppose we want to know the ‘power (of a particular design and testing procedure) to detect an effect size of 0.2 SD.’ Here we should generate data that reflects this effect size (for considering the true positive and false negative rates). If we also want to measure the rate of type-2 error, the rate of false positives (if the null, e.g., an effect side of 0, holds), we should also generate data reflecting this ‘null effect.’ Run the proposed analysis over the many simulated data sets (as efficiently as possible, as this can take a long time). This analysis should either return the estimand of interest, or ensure that we can easily compute it (in Step 3). The output should be kept as flexible as possible (while conserving computer memory). This will allow us to assess multiple inference goals on the same output.* *E.g., in a Bayesian analysis, we can get the full posterior distribution for an analysis so it can be assessed and summarised in many ways in step 3. Summarize the output returned in Step 2, computing the share of simulated data sets that meet different decision criteria or inference goals. (Example described in fold) E.g., we might simulate 1000 data sets based on an effect size of 0.3 standard deviations and perform a standard t-test of the difference between treatment and control for each of these. We might then find that for 743 of 1000 of these simulated data, the test ‘rejected the null hypothesis,’ suggesting a power of 74.3%. (Feel free to add other examples here.) This can show the likelihood of achieving a range of goals given various sample sizes and design and testing choices, including the rates of misleading conclusions. In frequentist analyses, it is possible that steps 2 and 3 occur together (e.g., the p-value is returned along with the other output). In my experience with typical Bayesian designs, these stages are best kept separate so that we can first do the more time-consuming Step 2, and then more freely explore the various options in Step 3. Assess the output and determine whether the proposed analyses and inference goals are realistic and likely to yield informative results. If not, one may need to think of alternative design or data-collection choices (including sample sizes, treatments, and treatment assignments) or inference goals, and return to Step 1.* * DR: I removed ‘ways of generating data’ because I thought it could be misleading. It seems wrong to first say, e.g., ‘we assumed the a standard normal distribution of outcomes with a standard deviation equal to that observed in prior trials’… and then say ‘but that didn’t have enough power so lets assume a more concentrated distribution.’ What I think you meant was consider other ways of actually collecting data or setting up the experiment that would lead to a reasonable expectation of a different data generating process, and then simulate and diagnose this new approach. Note that DD seems to have good tools for comparing and considering design variations. In the sections that follow, I will present some generally useful packages and functions for power analyses, including annotated code examples with particularly useful aspects highlighted. I will do this in the context of a frequentist power analysis for between-groups (=between-subject?) analysis.** ** We give this example is primarily because between-groups analyses run much more quickly than within- subjects analyses. Thus, these can be run without a timewasting headache on your own computer! We hope to add (and link or connect) further examples of common designs in future. 11.1.4 Possible pitfalls/misunderstandings of power analysis It is important to note that power analyses are not an ‘omniscient oracle.’ They might better be termed ‘power projections’ or ‘power estimates.’ Even if we estimate that we have 99.9% power for detecting some a particular effect, we may have specified our simulations in an unrealistic way. When we actually run an experiment, we might see (e.g.) far more underlying variation or measurement error than we predicted, leading our analyses to be fairly uninformative. Conversely, we might be overly conservative with our power analysis; perhaps a design we thought was ‘underpowered’ actually has a high probability of producing very compelling results.* * DR: I changed the wording here because if we diagnose the design as ‘underpowered’ we probably would choose not to run it in that form. I raise this only to point out that our power analyses might not always be perfect or be something of a blunt instrument. We should recognise they are not guarantees but more a way of determining the basic plausibility of achieving certain goals. Some other things to consider in a similar vein are presented below, and again the intention here is merely to frame our use of power analyses and recognise what sort of things I suspect they can or can’t help us with: When we see real data and real interesting patterns emerge, we are likely to go further in modeling and investigating these patterns than some of the more simple analyses and comparisons we conduct in the initial power analysis. I suspect power analyses are not so good for determining all the intricate, in-depth things we might plumb in a dataset. They are probably better at assessing the tractability of a broad inference goal. We might want to give some consideration to how far we wish to go in the initial data simulation step to think about all sorts of hypothetical data sets. As models become more complex, the number of different parameters that might vary - with possible effects on power - starts to balloon. E.g., even in a simple repeated-measures example, do we wish to vary not only the effect size but all sorts of different correlations from pre- to post-treatment within subjects? If we are simulating ordinal data, then power might change depending on how we initially suggest binning the outcomes, but there are infinitely many ways we might think the data might look… Discussion and consideration of how far we should go with these things is welcome and could be useful! (DR: I’m not sure what you are getting at here. Are you saying that ‘the space of designs and proposed analyses we can explore is extremely large, and we may need to make some ad-hoc choices to avoid this getting unmanageable?’_ 11.2 Concrete implementation of framework 11.2.1 Step 1: Generate a large number of data sets for the analysis Confirm that we can generate the basic data we want Because ultimately we might be generating some rather large data files, we increase the memory limit alotted to R first. I don’t believe there is any real cost to increasing this memory limit, so it is wise to do so as to avoid a function iterating many times over for a long time, but ending up without sufficient space to store the outcome: memory.limit(100000) ## [1] Inf Now we want to generate a hypothetical data set. In this case, we will be comparing 4 groups, each shown a different message in a between-subjects design (3 active vs. a control condition with a neutral message). Willem has some nice procedures for generating repeated-measures data in his walkthrough (https://www.willemsleegers.com/posts/simulation-based-power-analyses/) using the library MASS. I think there are likely to be other useful packages or techniques for generating simulated data for more complex designs or patterns of results that we could look into. Being better able to generate valid and informative data sets is probably one of the things that would improve our capacity for running informative power analyses the most. In this example, we will take advantage of the different groups to assess several different possible effect sizes. In a multiple regression, we would then be able to assess whether these different effect sizes come out as significantly different from our control group, and therefore our power to detect different sizes of effects. # tidyverse simply used for data wrangling and plotting library(tidyverse) four.group.datamaker &lt;- function(sim = 1, a = 0, b = .1, c = .2, d = .4) { # first a tibble (data frame) with 1500 ppts, with the different groups showing # effect sizes in Cohen&#39;s d of .1, .2, and .4 four.groups &lt;- tibble(a.control = rnorm(1500, a, 1), b.small = rnorm(1500, b, 1), c.medsmall = rnorm(1500, c, 1), d.medium = rnorm(1500, d, 1), sample.size = 1:1500) %&gt;% # turn the data into long form pivot_longer(cols = &#39;a.control&#39;:&#39;d.medium&#39;, names_to = &#39;group&#39;, values_to = &#39;response&#39;) %&gt;% # put cutpoints in the data to make it more similar to the ordinal responses we would get mutate(ordinal = case_when(response &lt; -1.5 ~ 1, response &lt; -.5 ~ 2, response &lt; .5 ~ 3, response &lt; 1.5 ~ 4, response &gt;= 1.5 ~ 5), # for the purposes of this demo we will not analyse it as ordinal as it takes longer # to run the regressions, but if you did so you would also want to make the response # a factor ordinal = as.factor(ordinal), sim = sim) return(four.groups) } # test that the function works to make one data set before making many! test.data &lt;- four.group.datamaker() ggplot(data = test.data) + geom_density(aes(x = response, fill = group), alpha = .3) + theme( aspect.ratio = 1 ) ggplot(data = test.data) + geom_histogram(aes(x = as.numeric(ordinal)), alpha = .6, position = position_dodge(), bins = 10) + facet_wrap(~group) + theme( aspect.ratio = 1 ) We can see from the plots that the function appears to be working. When developing a data set for the first time, one would usually go further with some ‘diagnostic’ checks to confirm that the data is behaving as you intended. For example, in Willem’s examples, he used the mvnorm function with ‘empirical = TRUE,’ so that the exact mean diffreences you specified are present in the data. This can then be confirmed wiht descriptive statistics. There might be all sorts of other diagnostics or plots we might check with other types of data that we generate. Efficiently generate many data sets Now we just need to run the function above many times over. This could be done using loops, but a very useful set of R functions in the tidyverse is the purrr package of map functions. Even better, a package called furrr is available to run such map functions in parallel to further reduce time. This doesn’t matter so much here because this will be quite quick anyway, but is important when we run the analyses over the many data sets. For furrr to do this, we need to tell it to plan for ‘multisession,’ and give it a seed number: library(furrr) plan(multisession) options &lt;- furrr_options(seed = 48238) # we will pass N = 500 simulations to the map function nsims &lt;- 1:500 # the map function will run our data-making function over 1000 simulations sim.data &lt;- future_map_dfr(.x = nsims, .f = four.group.datamaker, a = 0, b = .1, c = .2, d = .4, .options = options) # split the simulated data into the separate simulations sim.data &lt;- sim.data %&gt;% group_by(sim) %&gt;% group_split() Now, we have 500 simulated data sets representing our hypothetical outcome data, and can perform analyses on them: head(sim.data[[1]]) ## # A tibble: 6 × 5 ## sample.size group response ordinal sim ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; ## 1 1 a.control -0.698 2 1 ## 2 1 b.small -0.141 3 1 ## 3 1 c.medsmall -0.0593 3 1 ## 4 1 d.medium 0.244 3 1 ## 5 2 a.control -1.40 2 1 ## 6 2 b.small 0.176 3 1 11.2.2 Step 2: Run the proposed analysis over the many data sets and return the estimands of interest Before we run an analysis over the many data sets, we need to check that our models will return the estimands that we wish to make inferences from. For a Bayesian analysis, this step might involve dropping parts of the posterior that are not relevant (e.g., estimates of every participant intercept, which take too much space for what they are worth) and ensuring we get the parts we care about (e.g., we might insert some additional code to retrieve a posterior distribution for Cohen’s d, or simply the parameter estimate for the interaction). For Bayesian analyses I err on the side of getting as many of the main parameters as possible, because this is a very time consuming step. For frequentist analyses, there are also a range of estimands we might care about. For example, we might consider returning a p-value, or the upper and lower bounds for confidence intervals, r-squared estimates etc. Again, this depends on the goals one wishes to achieve and what one wants to make inferences about. The key point for any kind of analysis here is that you don’t want to forget an estimand that might be of interest and then have to re-run the entire analysis. In addition, you want to have the regression run on different sample sizes of the data, so that you can generate a power curve plot for the analysis, showing how your power changes according to increasing the sample size. In the example function below, we run a simple linear regression, predicting the response from group. linear.reg.maker &lt;- function(data, breaks) { # this function cuts the data set it is given into different sample sizes cut.samples &lt;- function(break.point, data) { cut.data &lt;- filter(data, sample.size &lt;= break.point) cut.data &lt;- mutate(cut.data, sample.size = break.point) return(cut.data) } data.cuts &lt;- map_dfr(.x = breaks, .f = cut.samples, data = data) # the data is split according to the sample size # to feed to the regression model data.cuts &lt;- data.cuts %&gt;% group_by(sample.size) %&gt;% group_split() # this function runs the regression run.reg &lt;- function(data) { four.group.form &lt;- as.numeric(ordinal) ~ 1 + group four.group.reg &lt;- lm(formula = four.group.form, data = data) # we extract confidence intervals for the parameters of interest ci99 &lt;- confint(four.group.reg, level = .99) ci95 &lt;- confint(four.group.reg, level = .95) # we create an output to show the confidence intervals around the effects # and some additional inference info, e.g., &#39;nonzero&#39; indicates whether # the lower bound of the CI excludes 0 or not. # &#39;width&#39; indicates the width of the confidence interval, # for assessment of precision output &lt;- tibble(group = c(&#39;small&#39;, &#39;medsmall&#39;, &#39;medium&#39;, &#39;small&#39;, &#39;medsmall&#39;, &#39;medium&#39;), interval = c(.99, .99, .99, .95, .95, .95), lower = c(ci99[[2,1]], ci99[[3,1]], ci99[[4,1]], ci95[[2,1]], ci95[[3,1]], ci95[[4,1]]), upper = c(ci99[[2,2]], ci99[[3,2]], ci99[[4,2]], ci95[[2,2]], ci95[[3,2]], ci95[[4,2]])) %&gt;% mutate(&#39;nonzero&#39; = case_when(lower &gt; 0 ~ 1, TRUE ~ 0), &#39;width&#39; = abs(upper - lower), &#39;sim&#39; = data[[1, &#39;sim&#39;]], &#39;cell.size&#39; = nrow(data)/4) return(output) } # run the regression function over the different sample sizes output &lt;- map_df(.x = data.cuts, .f = run.reg) return(output) } Once we have made and tested that our function works as intended and returns the values we want to make inferences from, we can run it over the many simluated data sets: t1 &lt;- Sys.time() linreg.output &lt;- future_map_dfr(.x = sim.data, .f = linear.reg.maker, breaks = seq(from = 150, to = 1500, by = 150)) t2 &lt;- Sys.time() t2 - t1 ## Time difference of 48.40096 secs The object ‘linreg.output’ is now a large dataframe, cataloguing whether or not certain inference thresholds were reached across the many simulations. In the third primary step, we can summarise and graphically display this information. 11.2.3 Step 3: Summarise the output returned in Step 2 to determine likelihood of achieving various inferential goals Now, we wish to ascertain how likely we are to achieve a range of inferential goals, depending on factors such as the sample size, the underlying effect sizes, or whatever else we varied in simulating our data and running our models. For this example, this is as simple as generating a summary of the output from Step 2: # group the data according to group, confidence interval, and size per group four.group.lin.summary &lt;- linreg.output %&gt;% group_by(group, interval, cell.size) %&gt;% # summarise the amount of times we get a CI greater than 0 summarise(.groups = &#39;keep&#39;, &#39;ci above 0 vs. control&#39; = sum(nonzero)/5) %&gt;% # change some factors for plotting mutate(interval = factor(interval, levels = c(&#39;0.95&#39;, &#39;0.99&#39;), labels = c(&#39;95% CI&#39;, &#39;99% CI&#39;)), &#39;Effect size&#39; = factor(group, levels = c(&#39;small&#39;, &#39;medsmall&#39;, &#39;medium&#39;), labels = c(&#39;Very small (.1)&#39;, &#39;Small (.2)&#39;, &#39;Medium (.4)&#39;))) And then plotting the resulting power curve: ggplot(data = four.group.lin.summary) + scale_x_continuous(limits = c(100, 1550), breaks = seq(from = 150, to = 1500, by = 150)) + scale_y_continuous(limits = c(0, 100), breaks = seq(from = 0, to = 100, by = 20)) + geom_hline(aes(yintercept = 80), linetype = &#39;dashed&#39;, size = .33, alpha = .25) + geom_hline(aes(yintercept = 90), linetype = &#39;dashed&#39;, size = .33, alpha = .25) + geom_path(aes(x = cell.size, y = `ci above 0 vs. control`, color = `Effect size`, group = `Effect size`), size = .66) + geom_point(aes(x = cell.size, y = `ci above 0 vs. control`, color = `Effect size`), size = 1.5) + labs(y = &#39;Power to detect a non-zero effect&#39;, x = &#39;Number of participants per condition (control group not included)&#39;) + scale_color_manual(values = c(&#39;#c10d0d&#39;, &#39;#7dc3c2&#39;, &#39;#dcc55b&#39;)) + facet_wrap(~interval) + theme( aspect.ratio = 1, panel.grid.major = element_line(colour = &quot;white&quot;, size = 0.33), panel.grid.minor = element_line(colour = &quot;white&quot;, size = 0.2), panel.background = element_rect(fill = &quot;grey96&quot;), axis.line = element_line(color = &#39;black&#39;, size = 0.375), axis.ticks = element_line(color = &#39;black&#39;, size = 0.5), text = element_text(color = &#39;black&#39;, family = &#39;Gill Sans MT&#39;, size = 9), axis.text = element_text(color = &#39;black&#39;, family = &#39;Gill Sans MT&#39;, size = 7), strip.background = element_blank() ) 11.2.4 Step 4. Assess the output and determine whether the proposed analyses and inference goals are realistic and likely to yield informative results. In Step 4, we use the information we have generated above to make substantive conclusions about the projected power of our experiment to detect certain effects, given certain underlying parameters. From this we can make recommendations as to experimental design. Based on the power curve plotted above, we can conclude that we would have a very high likelihood of detecting effects of .4 vs. a control group at even quite low sample sizes, and also a good possibility of detecting effect sizes of .2 at quite modest sample sizes. On the other hand, for the very small effect size, we would not be confident in detecting such a difference vs. a control group even at 1500 participants per group. If effect sizes of this size we what was expected in such an experiment, and it was crucial that they were detected if present, then we might consider going back to Step 1 and reconsidering our experimental design to include even more participants. "],["readme-methodology-statistics-design-surveys-experiments-data-analysis-methodology-protocols-and-templates.html", "12 README: methodology-statistics-design, ‘Surveys, experiments, data analysis: Methodology, protocols and templates’ 12.1 ‘Methods and RP Guidelines Bookdown’ 12.2 Folder structure, other resources in this repo", " 12 README: methodology-statistics-design, ‘Surveys, experiments, data analysis: Methodology, protocols and templates’ NOTE: this is now publicly hosted but not indexed. Please be careful not to share any confidential data or sensitive information. Proceeding from Slack discussion here 12.1 ‘Methods and RP Guidelines Bookdown’ 12.1.1 To contain… … Only things that we use, have used, want to use, or have been requested to address. To keep this manageable, don’t add content just because \"it’s in a typical syllabus’. See airtable topic list … some examples below 12.1.2 Proposed structure Moved to introduction chapter here (file: introduction_overview.Rmd) 12.1.3 Sources Integrate some content/organization from Reinstein’s “Metrics bookdown” RP Slack threads ‘Resources’ chapters in EA Market testing Gitbook Textbooks/online resources: … Articles: … 12.1.4 Using and building this Bookdown (proposed guidelines) Add stuff to the Bookdown in a concise way, once it’s at least minimally readable/useable. (See distinct branches discussed below). For non-core material: out-link to standalone pages/gists/blogdowns whatever and/or Bookdown ‘appendix’ sections and folding boxes for the non-core material Link to these resources in Guru, but don’t repeat this content there. If an ideal guide to ‘exactly what we want’ exist elsewhere, just ‘curate link it,’ Pete W-style, don’t re-write the wheel Key links for tips on using ‘these tools’ Reinstein’s tips (from the EA barriers project) Reinstein’s ‘bookdown+ template’ repo Yihui’s bookdown bookdown Happy git with R Pete’s walkthroughs: git 101 Advanced R, abridged 12.1.5 Repo branches (proposed) ‘main’: Decent looking, go-to ‘public’: same as ‘main’ but simplified for public sharing (if we want to) ‘work-in-progress’: Work in progress including discussion that you don’t want to add to ‘main’ yet 12.2 Folder structure, other resources in this repo "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
